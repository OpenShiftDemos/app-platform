= Extend to the edge and interconnect (30 mins)

The open hybrid cloud approach enables applications to run anywhere and connect easily using the same tools without compromising security and availability. Decentralized systems are needed now more than ever to reduce latency, improve user experience, resiliency, and reduce costs.

https://www.redhat.com/en/technologies/cloud-computing/service-interconnect[Red Hat Service Interconnect^] allows connectivity between clusters, enabling developers to quickly and easily connect their applications without overhead. Red Hat Service Interconnect is based on (https://skupper.io/index.html[Skupper^])


https://www.redhat.com/en/products/edge[Red Hat Edge^] It allows organizations to deploy workloads closer to the end user location, providing flexibility with a right-sized platform. Red Hat Device Edge has many flavors. You will work with the Single Node OpenShift (SNO).

== A retail use case
In the following example, we will explore a retail use case. Diverse industries can implement this proposed architecture or similar when two services or systems running on different platforms need to connect.

A retail store wants to install an application closer to the end user location to gather inventory information from that particular location.

image::module2/edge_interconnect.png[width=80%]

*Retail Store - Las Vegas*: This retail has a Single Node OpenShift (SNO) already setup.

*Retail Central Hub*: This is an online marketplace hosted on Red Hat OpenShift running on AWS. 


== Introduction

In this lab, you will deploy a service and a database in the Single Node OpenShift (SNO), expose it using Red Hat Service Interconnect to make it available for others.  

In your Red Hat OpenShift running on AWS you will deploy the rest of the {app_name}. The catalog service will connect to the inventory service to gather information about inventory.

== Catalog on Central Hub - OpenShift on AWS

Red Hat Service Interconnect is available through the operator or command line. In This cluster, the operator is already installed.

* Login in the OpenShift using the terminal:

[source,sh,subs="attributes",role=execute]
----
{login_command}
----

* Create a new project to deploy the {app_name}. Run the following command in the terminal:

[.console-input]
[source,sh,subs="attributes",role=execute]
----
oc new-project coolstore-{user}
----

* Explore the yaml file that will deploy the catalog and the database.

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
cd ~/app-platform/content/modules/ROOT/files/module-02
----

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
cat coolstore.yaml
----

* Deploy the rest of the {app_name}, by running the following commands in the terminal:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc apply -f coolstore.yaml
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output:
      serviceaccount/catalog-app created
      secret/catalog-database created
      deployment.apps/catalog-database created
      service/catalog-database created
      deployment.apps/catalog created
      service/catalog created
      route.route.openshift.io/catalog created
      secret/order-placement created
      serviceaccount/order-placement created
      deployment.apps/order-placement created
      service/order-placement created
      serviceaccount/globex-app-globex-ui created
      deployment.apps/globex-ui created
      service/globex-ui created
      route.route.openshift.io/globex-ui created
----

* Create an instance of Red Hat Service Interconnect (RHSI).

RHSI is already installed as an Operator at the cluster level. We need to create an instance of this operator in your namespace. We create an instance by creating a ConfigMap with the name **skupper-site**. 

https://skupper.io/docs/declarative/index.html[This  method is called declarative approach^]

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
cat configmap_rhsi.yaml
----

**Notes**: The ConfigMap will determine the features available for you. In this case, we enabled **console: "true"** to access the UI.

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc apply -f configmap_rhsi.yaml
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      configmap/skupper-site created
----

* Verify that all pods are running:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get pods -w
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output:
      NAME                                          READY   STATUS    RESTARTS      AGE
      catalog-77b478948c-jxlqn                      1/1     Running   0             55s
      catalog-database-985996745-gmlnd              1/1     Running   0             55s
      globex-ui-79f448fd99-v8qzt                    1/1     Running   0             54s
      order-placement-689c64c679-m6fm4              1/1     Running   0             54s
      skupper-prometheus-76f469b48d-k88sj           1/1     Running   0             32s
      skupper-router-7cfb8958f-gg2w2                2/2     Running   0             35s
      skupper-service-controller-5bf78d86c6-zjm5h   2/2     Running   0             33s 
----
Note: Wait until all the pods are running, as shown above.

* Create the secret required by Red Hat Service Interconnect to access the current namespace. This token is linked to the secret created recently:


[.console-input]
[source,bash,subs="+attributes,macros+"]
----
cat secret-interconnect.yaml
----

[.console-output]
[source,bash,subs="+attributes,macros+"]
----
apiVersion: v1
kind: Secret
metadata:
  labels:
    skupper.io/type: connection-token-request
  name: secret-interconnect
----

** Create the secret:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc apply -f secret-interconnect.yaml
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      secret/secret-interconnect created
----

** Create the token required to access the current namespace. This token is linked to the secret created recently:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get secret -o yaml secret-interconnect > token.yaml
----

* Edit the yaml file to remove the namespace:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
vi token.yaml
----

* press *i* to edit the file
* remove the namespace from the file:
  labels:
    skupper.io/type: connection-token
  name: secret-interconnect
  resourceVersion: "440300"
  uid: df3ec97e-de2d-4e7c-9099-21aa99858f92
type: Opaque

* Save the file by pressing ESC + :wq! + ENTER

* From a web browser, access the {app_name} application using the application route:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get route
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      NAME        HOST/PORT                                                    PATH   SERVICES    PORT   TERMINATION     WILDCARD
      ....
      globex-ui   globex-ui-demo.apps.cluster-hpxfn-1.sandbox933.opentlc.com          globex-ui   http   edge/Redirect   Non
----
* Copy the globex-ui URL in your web browser.
* On the main menu, select the tab **Cool Stuff Store**

image::module2/coolstore-inventoryissue.png[width=80%]


**Notes**: 

The inventory information is missing but the {app_name} website is still visible. Once access to the inventory service this will be resolved. In real situations, we want the {app_name} to be idempotent against losing access to the inventory service/database.

== Inventory on Retail Store - (SNO)

* Login in the SNO using the terminal:

[.console-input]
[source,sh,subs="attributes",role=execute]
----
oc login -u {user} https://api.cluster-CLUSTER_ID.opentlc.com:6443 --insecure-skip-tls-verify=true
----

* Create a new project to deploy the PostgreSQL database and service, *inventory*. Run the following command in the terminal:

[.console-input]
[source,sh,subs="attributes",role=execute]
----
oc new-project inventory-{user}
----

* Explore the yaml file that will deploy the database.

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
cat inventory-all.yaml
----

* Deploy the inventory database and backend service by running the following commands in the terminal:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc apply -f inventory-all.yaml
----

[.console-output]
[source,subs="+attributes,macros+"]
----
output:
      serviceaccount/inventory-app created
      secret/inventory-database created
      deployment.apps/inventory created
      deployment.apps/inventory-database created
      service/inventory-database created
----

* RHSI is installed as an Operator. You will be creating an instance of the Operator using a ConfigMap.

** Execute the following command on your terminal:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc apply -f configmap_rhsi_sno.yaml
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      configmap/skupper-site created
----

* Verify that all pods are running:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get pods
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      NAME                                          READY   STATUS    RESTARTS   AGE
      inventory-67fffc6d57-94v8j                    1/1     Running   0          2m42s
      inventory-database-5f4565cc5f-strc2           1/1     Running   0          2m42s
      skupper-router-7b787c887f-pwclh               2/2     Running   0          9s
      skupper-service-controller-7f6fb474ff-t4zkl   1/2     Running   0          7s
----
* Wait until all the pods are running.
* Create the token required by Red Hat Service Interconnect to access the {app_name}'s namespace. This token is linked to the secret created recently:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc apply -f token.yaml
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      secret/secret-interconnect created
----

* Expose the deployment will create a service to be accessible to the connected sites. 

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc annotate deployment/inventory skupper.io/proxy="http"
----

[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      deployment.apps/inventory annotated
----
** Explore the new services created by skupper related to Skupper functionality and inventory services to connect pods with the service.

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get services
----

[.console-output]
[source,subs="+attributes,macros+"]
----
output:
      NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)               AGE
      inventory              ClusterIP   172.31.27.181    <none>        8080/TCP              9s
      inventory-database     ClusterIP   172.31.90.129    <none>        5432/TCP              91s
      skupper                ClusterIP   172.31.53.198    <none>        8081/TCP              97s
      skupper-router         ClusterIP   172.31.248.104   <none>        55671/TCP,45671/TCP   98s
      skupper-router-local   ClusterIP   172.31.9.18      <none>        5671/TCP              98s
----

* Verify that the application is fully functional with the inventory information:

** Go to the OpenShift on AWS
** Access the route 

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get routes
----

image::module2/coolstore_fullworking.png[width=80%]

The inventory information is available now.

== Explore services and connections with Red Hat Service Interconnect UI (Central Hub - OpenShift on AWS)

* Using your web browser access the Red Hat Service Interconnect UI:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get route
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
...
skupper                skupper-demo-service.apps.cluster-rqk9v.rqk9v.sandbox1343.opentlc.com                       skupper          metrics        reencrypt/Redirect     None
----
* User: admin Pass: ocp123
* Explore the different components

** **Topology**: Graphical representation of all the connections

Two sites were created: Hub and retail location.

image::module2/rhsi_sites.png[width=80%]


** **Addresses**: The exposed deployment is shown here and available to connect using the specific address.

image::module2/rhsi_addresses.png[width=80%]

*** Click on the service:
 - Throughput Bytes: Charts providing traffic related information

The database will show receiving and sending traffic to the {app_name} site.

image::module2/rhsi_traffic.png[width=80%]

** **Components**: Services that are exposed on the service network, both local and remote.

** **Sites**: Application Interconnect installations on the current service network.

Two sites will be visible, from the SNO (retail-location) and OpenShift (retail-hub)


== Bonus point using a declarative approach:
In this section, you will move the database to a new namespace and to connect the database with the inventory service; this section follows a declarative approach; we do not need the skupper CLI.

* Remove the database from the current namespace

** Start by deleting the database components.

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc delete deployment inventory-database
oc delete service inventory-database
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      deployment.apps "inventory-database" deleted
      service "inventory-database" deleted
----

** From a web browser, access the {app_name} application using the application route:
***Note: Verify all related pods are terminated.

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get pods
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      NAME                                          READY   STATUS    RESTARTS   AGE
      inventory-f6957f6d5-j5l2t                     1/1     Running   0          8m48s
      skupper-router-6b96d8fd57-7b55h               2/2     Running   0          8m34s
      skupper-service-controller-754f79b94d-mx4h5   1/1     Running   0          8m32s
----

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get route
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      NAME        HOST/PORT                                                    PATH   SERVICES    PORT   TERMINATION     WILDCARD
      ....
      globex-ui   globex-ui-demo.apps.cluster-hpxfn-1.sandbox933.opentlc.com          globex-ui   http   edge/Redirect   Non
----

** Create the secret required to access the current namespace:

** Execute the following command on your terminal:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
cat secret-interconnect-database.yaml
----

[.console-output]
[source,bash,subs="+attributes,macros+"]
----
apiVersion: v1
kind: Secret
metadata:
  labels:
    skupper.io/type: connection-token-request
  name: secret-interconnect-database
----

** Create the secret:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc apply -f secret-interconnect-database.yaml
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      secret/secret-interconnect-database created
----

** Create the token required to access the current namespace. This token is linked to the secret created recently:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get secret -o yaml secret-interconnect-database > token-database.yaml
----

* Edit the yaml file to remove the namespace:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
vi token-database.yaml
----

* press *i* to edit the file
* remove the namespace from the file:
  labels:
    skupper.io/type: connection-token
  name: secret-interconnect-database
  resourceVersion: "386275"
  uid: 547c1b17-faf8-4fc8-9a58-26d4faa531ec
type: Opaque


* On the main menu, select the option **Cool Stuff Store**

image::module2/coolstore-inventoryissue.png[width=80%]

Note: You will notice that the inventory information is again not available.

* Recreate the database component in a new namespace.
** Create a new project to deploy the PostgreSQL database , *inventory-database*. Run the following command in the terminal:

[.console-input]
[source,sh,subs="attributes",role=execute]
----
oc new-project inventory-database-{user}
----

* Explore the yaml file that will deploy the database.

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
cd $HOME/app-platform/content/modules/ROOT/files/module-02
----

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
cat inventory-database.yaml
----

* Deploy the inventory database and backend service by running the following commands in the terminal:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc apply -f inventory-database.yaml
----

[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      serviceaccount/inventory-app created
      secret/inventory-database created
      deployment.apps/inventory-database created
----

* Create a new instance of RHSI using the ConfigMap:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc apply -f configmap_rhsi_sno_database.yaml
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      configmap/skupper-site created
----

* Verify that all pods are running:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get pods
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      NAME                                          READY   STATUS    RESTARTS   AGE
      inventory-database-574fc65f75-wmdfx           1/1     Running   0          94s
      skupper-router-85fc9b4c5c-k8rvk               2/2     Running   0          55s
      skupper-service-controller-6c498884bf-tg7gv   1/1     Running   0          54s
----
* Wait until all the pods are running.
* Create the token required to access the current namespace:

** Explore the yaml file:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
cat token-database.yaml
----

[.console-output]
[source,bash,subs="+attributes,macros+"]
----
apiVersion: v1
kind: Secret
metadata:
  annotations:
    ...
  labels:
    skupper.io/type: connection-token-request
  name: secret-interconnect-database
type: Opaque
----

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc apply -f token-database.yaml
----
[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      secret/secret-interconnect-database created
----

* Expose the database deployment will create a service to be accessible to the connected sites. 

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc annotate deployment/inventory-database skupper.io/proxy="tcp" "skupper.io/port=5432"
----

[.console-output]
[source,subs="+attributes,macros+"]
----
output: 
      deployment.apps/inventory-database annotated
----

* Verify that the application is fully functional with the inventory information:

** Go to the OpenShift on AWS
** Access the route 
[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get routes
----

image::module2/coolstore_fullworking.png[width=80%]

The inventory information is now available.


*Note: If you don't see the inventory information yet. Review the inventory logs and restart the pod.

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc get pods
----

* Use the inventory-database pod's name for the 
following commands. For example:


[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc logs inventory-f6957f6d5-7tggx
----

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc delete pod inventory-f6957f6d5-7tggx
----


* Explore the different components in RHSI:

** **Topology**: Graphical representation of all the connections

A new site was created: Hub and retail location.

image::module2/rhsi_topologies_3.png[width=80%]


** **Addresses**: The exposed deployment is shown here and available to connect using the specific address.

image::module2/rhsi_addresses_2.png[width=80%]

** Click on the *Process tab* in the Topology view
** Select the checkbox *show metrics*

image::module2/rhsi_processes_withtraffic.png[width=80%]

Note: if you don't see any traffic refresh the coolstore webpage again.

== Conclusion

**Congratulations on finishing this module!**

In this module, you connected two services, one residing on OpenShift on the cloud and the other on Singe Node OpenShift (part of the Red Hat Edge offerings). 

The applications were connected using Red Hat Service Interconnect. RHSI allows connectivity between multiple clusters but also at the edge. 

The bonus point module shows how RHSI can be available within the same cluster. In this section, you have connected a database in a namespace with the backend services in another namespace.

== More Information:

=== Red Hat Edge

* https://www.redhat.com/en/products/edge[Red Hat Edge^]

* https://www.redhat.com/en/technologies/cloud-computing/openshift/edge-computing[Edge computing with Red Hat OpenShift^]

* https://www.redhat.com/en/technologies/device-edge[Red Hat Device Edge^] 

* https://developers.redhat.com/articles/2023/11/16/red-hat-openshifts-flexibility-our-topologies-your-topographies[Red Hat OpenShift's flexibility: Our topologies for your topographies^]

* https://developers.redhat.com/articles/2023/11/14/red-hat-edge-platforms-more-options-more-use-cases[Red Hat Edge Platforms: More options for more use cases^]

=== Red Hat Service Interconnect
* https://github.com/skupperproject[Interconnect: More use cases and examples^]
* https://skupper.io/docs/declarative/index.html[RHSI Declarative approach^] 
* https://skupper.io/start/index.html[RSHI command line^] 
